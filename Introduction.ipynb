{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Machine Learning?\n",
    "Machine Learning is about building computational artifacts that learn over time based on experience. You have data and you try and infer information from that data using computationally applied statistical algorithms. It is a field of study that gives computers the ability to learn without being explicitly programmed. \n",
    "\n",
    "Here's a more formal definition:\n",
    "\n",
    "_A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E._\n",
    "\n",
    "Let's break this down in terms of an example. Let's say you want to create a spam mail classifier. How will that problem look like in terms of the above definition?\n",
    "\n",
    "- Task: Classifying mail as spam or not spam.\n",
    "- Experience: Watching us interact with the mailbox by throwing some mails in the junk folder and keeping the rest.\n",
    "- Performance: How many mails does our classifier correctly mark as spam/not spam.\n",
    "\n",
    "There isn't a simple algorithmic solution to this problem. You might think that most spam mails follow a certain kind of pattern or have certain tells which you can hardcode into an algorithm but that really isnt true anymore. Many spam mails are quite sophisticated now and dont really follow any set pattern. In fact, one person's spam could be another person's steak! So really the pattern we'll be looking for here is quite arbitary and is actually dependent on the person who is using the mailbox. This is wear statistical learning comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Supervised learning can be thought of as **function approximation**. You have a bunch of data and labels corresponding to that data and you want to computationally approximate the most accurate relationship between the data and its labels anda then generalize the relationship so that you can accurately predict the labels of new datasets too.\n",
    "\n",
    "_Example:_\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td><b>Data (Input)</b></td>\n",
    "        <td><b>Label (Output)</b></td>\n",
    "    </tr>\n",
    "<tr>\n",
    "    <td>1</td><td>1</td>\n",
    "</tr>\n",
    "<tr><td>2</td><td>4</td></tr>\n",
    "<tr><td>3</td><td>9</td></tr>\n",
    "<tr><td>4</td><td>16</td></tr>\n",
    "<tr><td>5</td><td>25</td></tr>\n",
    "<tr><td>6</td><td>36</td></tr>\n",
    "</table>\n",
    "\n",
    "Can we _approximate_ what the function behind this dataset is? Looks like $x^2$, doesnt it? Based on that can we predict what the output for the following input is:\n",
    "\n",
    "$$10 - ?$$\n",
    "\n",
    "Since, the function is $x^2$, the output will be:\n",
    "\n",
    "$$10 - 100$$\n",
    "\n",
    "However, this is still a leap of faith. We _assume_ the function is well behaved and is $x^2$ but what if the function is actually something like this:\n",
    "\n",
    "$$  f(x) =  \\begin{cases}\n",
    "      if x < 10: x^2\\\\\n",
    "      else: x\n",
    "    \\end{cases}       \n",
    "$$\n",
    "\n",
    "In this case, our prediction for input $10$ is incorrect. Therefore, there are a few assumptions with which we approach supervised learning, one of them being that the function behind the data is a well behaved function. A well behaved function is continous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "Unlike supervised learning, in unsupervised learning we just have the inputs, without any outputs. We dont have the labels for our data and we have to derive some structure for the data using just the inputs. \n",
    "\n",
    "_Example_:\n",
    "You might be given a bunch of fruits without information about what each fruit is called (let's assume you dont know anything about fruits). And you can still start classifying them in groups based on shape or color or smell. Let's say you group them up by color, you put all orange color fruits in one group, all red colored fruits in one, and all yellow colored fruits in one. You have derived a structure between the fruits (based on color) by using just the data.\n",
    "\n",
    "So if supervised learning is about functional approximation, unsupervised learning is about **description**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "Learning from delayed rewards. Supervised learning is about playing a game _after_ learning about all the rules. So if you're playing tic tac toe, you know exactly where to move and what move is a good move and what move is a bad move. Reinforcement learning is playing a game _without_ knowing any of the rules, and discovering the best way to play the game after you have finished playing it. Again, if you're playing tic tac toe without knowing what it is and you keep playing multiple games of tic tac toe, you'll start discovering the various rules and moves of tic tac toe on your own and after a significant amount of play time, you'll become very good at tic tac toe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these forms of learning can be thought of as some form of **optimizations**.\n",
    "- In supervised learning you're trying to optimize a funtion that labels data well.\n",
    "- In unsupervised learning you're trying to find the most optimal way to group together data.\n",
    "- In reinforcement learning you're trying to optimize a certain behavior.\n",
    "\n",
    "In machine learning, **data** is central. It differs slightly from a pure AI perspective where **algorithms** become central. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a Machine Learning Project Look Like\n",
    "1. **Define and Assess the Problem**: The first aim in any project, almost unanimously across industry, is to know what you're doing. The same applies to machine learning/data science/statistical analysis projects. Formulate the question/problem/value you are trying to answer/solve/add. Assess the problem. _Problems before requirements_, _requirements before solutions_, _solutions before design_, and _design before technology_.\n",
    "\n",
    "\n",
    "2. **Gather the Data**: After defining the problem you're trying to solve and assessing its validity, we move on to actually looking at the data requirements. If you have correctly defined and assessed the problem, then you should know what kind of data you are looking for. For example, if you've defined your problem as being able to correctly predict the prices of houses in X locality; then maybe you've identified the data which can help you predict the prices as number of rooms in a house, types of rooms in a house, total area, location, nearby ammenities like schools and hospitals, crime in area and so on. These are also known as _features_ or _independent/predictor/input variables_ and there are different ways to select the best features. Of course, you'll also need the past prices of these houses to map the features onto. The past prices of the houses is called the _dependent/output variable_; or the _response variable_. So the next step will be to either find the data online or go and collect it.\n",
    "\n",
    "\n",
    "3. **Prepare Data for Consumption**: This step is often referred to as _data wrangling_, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n",
    "\n",
    "\n",
    "4. **Exploratory Analysis**: At this stage the idea is to understand our data better. This involves using statistics to describe our data, the distribution, the variance and standard deviations, it's spread, correlations etc and then infer knowledge from it. Here we also check for multicollinearity, if we can perform any kind of feature engineering&mdash;be it creating new features out of the set we have, reducing features; or selecting the best ones for modelling. Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO).\n",
    "\n",
    "5. Model Data: Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.\n",
    "\n",
    "6. Validate and Implement Data Model: After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our model overfit, generalize, or underfit our dataset.\n",
    "\n",
    "7. Optimize and Strategize: This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange\" rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Vs Classification\n",
    "- A person's age, height, or income, the value of a house, or the price of a stock are all examples of continuous variables. Problems which map continuous inputs to outputs are known as regression problems.\n",
    "\n",
    "- Discrete variables take on values in one of $K$ different classes, or categories. Like a person's gender, the brand of a product purchases, cancer diagnosis, or you can also convert a continuous variable to a discrete one. Problems involving discrete outputs are known as classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
