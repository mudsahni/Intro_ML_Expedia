{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surviving the Sinking of the Titanic\n",
    "\n",
    "## Problem Statement\n",
    "We want to develop an algorithm that can look at the passenger data and help us predict the survival of those passengers.\n",
    "\n",
    "## Logistic Regression\n",
    "Rather than modelling the response Y (default) directly, logistic regression models the _probability_ that Y belongs  to a particular category. For this dataset, we'll use logistic regression to model the probability of passenger 'survival' given a host of predictor variables, $X$.\n",
    "\n",
    "$$ \\text{Pr}(\\text{default} = \\text{Yes | X})$$\n",
    "\n",
    "The values of $p(\\text{X})$ will range between 0 and 1. Then for any given value of X, a prediction can be made for survival. \n",
    "\n",
    "### The Logistic Model\n",
    "Our aim is to use the linear regression equation:\n",
    "\n",
    "$$ p(X) = \\beta_0 + \\beta_1\\times X_1 + \\beta_2\\times X_2 + \\ ... \\ + \\beta_n \\times X_n$$\n",
    "\n",
    "to model the probability of finding defaulters given their balance. To do this we must model $p(X)$ using a function that gives outputs between 0 and 1 for all values of X. Many functions meet this description. In logistic regression, we use the **logistic function**. \n",
    "\n",
    "$$p(X) = {1 \\over {1 + e^{-\\beta_0 + \\beta_1X_1 + \\ ...}}}= {e^{\\beta_0 + \\beta_1 X_1 + \\ ...} \\over {1 + e^{\\beta_0 + \\beta_1 X_1 + \\ ...}}}$$\n",
    "\n",
    "The logisitc function is also known as the **sigmoid function**. A sigmoid funtion has a characteristic S-shaped curve, which means it oscillates between 0 and 1. We can convert our linear equation values into a probability using a sigmoid function. These probability values can then be used to give us a 0 or 1 binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data and numerical computing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Helper function for feature engineering and modelling\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization Libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Setting some default plotting options\n",
    "mpl.style.use('seaborn')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette(\"cubehelix\")\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Write the sigmoid function. The sigmoid function accepts a number or a matrix and returns the sigmoid of the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the logistic function\n",
    "def sigmoid_logistic(x):\n",
    "    '''\n",
    "    Arguments:\n",
    "        x -- Integer or matrix of shape (m,n)\n",
    "    Output:\n",
    "        Returns the sigmoid of the product\n",
    "    '''\n",
    "    #TODO\n",
    "    sigmoid = ?\n",
    "    return sigmoid\n",
    "\n",
    "#TEST\n",
    "np.random.seed(420)\n",
    "a = np.random.randn(3,3)\n",
    "a = sigmoid_logistic(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should get the following output**:  \n",
    "```\n",
    "[[0.38097996, 0.12952627, 0.37139852 ]  \n",
    "[0.42224943,  0.58193127,  0.71880409]  \n",
    "[0.36164969, 0.4517072,  0.78798213]] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an array\n",
    "x = np.random.normal(0,3,10000)\n",
    "x.sort()\n",
    "#Using the sigmoid on the array\n",
    "z1 = sigmoid(x)\n",
    "#Plot\n",
    "f = plt.figure(figsize=(8,5))\n",
    "plt.plot(x,z1)\n",
    "plt.axhline(0.5, color = \"green\")\n",
    "plt.legend([\"Sigmoid\",\"Decision Boundary\"])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sigmoid\")\n",
    "plt.title(\"Logistic Function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit of manipulation of the sigmoid function:\n",
    "$${p(X) \\over {1-p(X)}} = {e^{\\beta_0 + \\beta_1X}}$$\n",
    "\n",
    "The quantity ${p(X) \\over {1-p(X)}}$ is called the _odds_, and can take on any value between $0$ and $\\infty$. Values of the odds close to 0 and âˆž indicate very low and very high probabilities of default, respectively.\n",
    "\n",
    "By taking the logarithm of both sides:\n",
    "$${\\log{\\large(}{p(X)\\over{1-p(X)}}{\\large)}} = \\beta_0 + \\beta_1X$$\n",
    "\n",
    "The left hand side is called the _log-odds_ or **logit**. \n",
    "\n",
    "In linear regression, $\\beta_1$ gives the average change in $Y$ associated with one unit increase in $X$. In logistic regression, increasing $X$ by one unit changes the logit by $\\beta_1$, or equivalently it multiplies the odds by $e^{\\beta_1}$. However, because the relationship between $p(X)$ and $X$ is not a straight line, $\\beta_1$ does not correspond to the change in $p(X)$ associated with one unit increase in $X$. The amount that $p(X)$ changes due to a one-unit change in $X$ will depend on the current value of $X$. But regardless of the value of $X$, if $\\beta_1$ is positive then increasing $X$ will be associated with increasing $p(X)$, and if $\\beta_1$ is negative, then increasing $X$ will be associated with decreasing $p(X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Start with the Data!\n",
    "\n",
    "Now there are many optimization algorithms for Logistic Regression. The most basic one is called **Maximum Liklihood Estimation** but we are not going to go into it right now. In fact, we'll use another optimization algorithm called **Gradient Descent**. But first, we'll start working with our data and use Python's machine learning libraries to model our data using Logistic Regression (we'll start by treating the optimization algorithm as black box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the data\n",
    "train = pd.read_excel(\"data/titanic/train.xlsx\")\n",
    "test = pd.read_excel(\"data/titanic/test.xlsx\")\n",
    "#Let's combine them both into one dataset for now\n",
    "data = pd.DataFrame.append(other = test, self= train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of passengers in dataset: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a glimpse at the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see some summary statistics about the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding our Independent and Dependent Variables\n",
    "\n",
    "- _Survived_: This is obviously our dependent variable. We want to potentially use the rest of the data to accurately predict the surivival of the passengers.\n",
    "\n",
    "\n",
    "- _PassengerID_ and _Ticket_ are random unique identifiers and we can assume that they will not have an impact on the survival rate of the passengers.\n",
    "\n",
    "\n",
    "- _Pclass_: Stands for passenger class, it is an ordinal variable (has a hierarchy to the value) and we can assume that it represents the socio-economic class of the passenger. \n",
    "\n",
    "\n",
    "- _Sex_: Gender of the passenger, nominal (categorical) variable. We will one-hot-encode this during the modelling process.\n",
    "\n",
    "\n",
    "- _Embarked_: Port from where the passenger boarded the Titanic. Categorical variable. Eg. C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "\n",
    "- _Age_ and _Fare_: Continous quantitative variables.\n",
    "\n",
    "\n",
    "- _SibSp_ and _Parch_: Represents number of related siblings/spouse aboard and number of related parents/children aboard respectively.\n",
    "\n",
    "\n",
    "- _Cabin_: Cabin number on the ship. Categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "### Checking for null values\n",
    "\n",
    "Check if there are any null values present in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values in any of the columns\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite a few missing values to deal with. Let's deal with all of them one by one. \n",
    "\n",
    "1. **Age**: In such a small dataset as this we cannot afford to not drop the missing rows and/or not use them in our analysis. The best way to go about this is to first see what kind of distribution Age is following. Most of the times variables like Age in a setting like the Titanic will follow the Normal Distribution, which means that most of the data will be centered around the mean (mean and median are the same in a Normal Distribution). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the distribution of Age\n",
    "data['Age'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, it is a normal distribution with a bit of a skew. Our mean and media should be nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Age on the Titanic: {data['Age'].mean()}\\n\\\n",
    "Median Age on the Titanic: {data['Age'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the median or rounded of mean to fill in the null values for Age.\n",
    "\n",
    "2. **Cabin**: With almost 80% of the data missing for Cabin, this column is useless for our analysis and we can just drop it. \n",
    "\n",
    "3. **Embarked**: Only 4 missing rows. This is a categorical column so we can't use mean/median here but we can use mode. Mode is the most commonly occuring value.\n",
    "\n",
    "**Exercise**  \n",
    "1. Replace missing values for _Age_ with its median value.\n",
    "2. Replace missing values for _Embarked_ with its mode.\n",
    "3. Drop the column for _Cabin_, _PassengerID_, and _Ticket_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Most Common Value in Embarked: {data['Embarked'].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN values\n",
    "\n",
    "#TODO: Replace NaN values for Age\n",
    "?\n",
    "\n",
    "#Replace NaN values for Embarked\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "#Drop Cabin, PassengerID, and Ticket\n",
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "data.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the null situation looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**  \n",
    "If you've implemented the missing value fix correctly, then you should not see any null values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Exploratory Analaysis\n",
    "Let's have a look at all our predictors and see how they play with eachother, what kind of information we can get from them and if we can infer some new features out of them.\n",
    "\n",
    "**Exercise**  \n",
    "Examine the rate of survival based on Pclass, Sex, and Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the dependent variable survived based on Pclass\n",
    "data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Group the rate of survival based on Sex\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Female passengers had a much better chance at survival than male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Now group the dependented variable survived based on both Pclass and Sex \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this distribution it seems that the passengers who were economically better off had a higher chance of survival. Drilling down on the passengers by gender we see that in every passenger class group, female passengers had a much higher chance of survival than male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Group rate of survival based on Embarked\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, people who boarded from Cherbourg had a better rate of survival than the other two ports. We'll examine this later on.\n",
    "\n",
    "**Exercise 4**  \n",
    "Let's now try and find information about survival based on Age, Fare, and the number of siblings/parents with each passenger. We'll have to modify our approach a little bit for this. \n",
    "1. We can group different ages/fares together into categories and then check the survival rate for those categories.\n",
    "2. After grouping passengers based on age group, try and see if gender is a good variable get more granular with the division.\n",
    "3. Similarly, we can use the information we have about siblings/parents and create a new feature called _'FamilySize'_ and then check the survival rate for FamilySize. Based on this approach we can also see if a passenger is travelling alone and create a new feature for that as well. \n",
    "4. Group the data by different Fare prices and check the difference in survival rates.\n",
    "\n",
    "_Hint: Remember that when creating family size, add 1 to include the passenger themselves._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with Age\n",
    "\n",
    "#Create a new column for age group on the basis of the following divisions\n",
    "# <= 14 - Child\n",
    "# <= 18 - Adolscent\n",
    "# <= 40 - Adult\n",
    "# <= 60 - Middle Aged\n",
    "# > 60 - Elderly\n",
    "\n",
    "#Precise way to do it\n",
    "data['AgeGroup'] = np.where(data['Age'] <= 13, 'Child', \n",
    "                   (np.where(data['Age'] <= 18, \"Adolescent\", \\\n",
    "                    np.where(data['Age'] <= 40, \"Adult\", \\\n",
    "                    np.where(data['Age'] <= 60, \"Middle Aged\", \"Elderly\")))))\n",
    "data[['AgeGroup','Survived']].groupby(['AgeGroup'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from this that the best chances of survival was with the very young and the worst with the elderly. This makes sense because children wouldve been looked after by their parents, siblings and other adults and would have been the first to be loaded onto rafts and boats.\n",
    "\n",
    "Let's see if we drill down by gender then what changes do we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Group the data together by AgeGroup and Sex\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that gender played a big role in determining survival rate. Chivalry was not dead a 100 years ago and the old adage of \"Women and Children first\" rang true in the case of the Titanic. Almost at every level, except children, the difference between survival rate for female passengers and male passengers is astronomical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Now let's create the vector for FamilySize\n",
    "#The vector for FamilySize should be a sum of the vector for siblings and parents\n",
    "#Dont forget to include the passenger\n",
    "?\n",
    "\n",
    "#Now group the survival rate by FamilySize\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Isolate passengers who are travelling alone and measure survival rates\n",
    "#Hint: you can use np.where\n",
    "?\n",
    "\n",
    "#Group survival rate by isAlone\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Further group survival rate by lone passengers and gender\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Large family sizes had a poorer rate of survival than mid-sized families.  \n",
    "2. Best rate of survival seems to be in a family of 4.  \n",
    "3. Passengers travelling alone had a lower rate at survival than passengers travelling with family.\n",
    "4. Even in alone passengers, female travellers had a much higher survival rate than male travellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use the data for fare cut by 4 quarters \n",
    "data['Fare'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping by survival rate\n",
    "data['FareCategory'] = np.where(data['Fare'] <= 7, \"Cheap\", \n",
    "                   (np.where(data['Fare'] <= 14,  \"Lower-Mid\", \\\n",
    "                    np.where(data['Fare'] <= 31, \"Upper-Mid\", \"High\"))))\n",
    "\n",
    "\n",
    "data[['FareCategory','Survived']].groupby(['FareCategory'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only feature we havent really fiddled with is the Name of the passenger. Generally, that would not provide us much information to use but in this case, all the names are appended with a title. Perhaps we can extract some information from the titles of our passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library for regular expressions\n",
    "import re\n",
    "\n",
    "#Function to use a regular expression to find titles from names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "#applying the function to the Name column\n",
    "data['Title'] = data['Name'].apply(get_title)\n",
    "\n",
    "#looking at the distribution for titles cut by gender\n",
    "pd.crosstab(data['Title'], data['Sex'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's categorize all these titles into broader groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group together uncommon titles into one category\n",
    "data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n",
    "\n",
    "#Replacing the title abberations in titles\n",
    "data['Title'] = data['Title'].replace('Mlle', 'Miss')\n",
    "data['Title'] = data['Title'].replace('Ms', 'Miss')\n",
    "data['Title'] = data['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "#Group survival rate by title\n",
    "data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most of this information was known to us through the use of age groups and gender, we have managed to get an additional feature for 'Other' titles and the fact that they add on to the rate of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the data\n",
    "\n",
    "With so many categorical variables, we will need to encode them so that we can use them in our predictive model. First we'll encode the text categories into numerical categories and then later on we will use one-hot-encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of the dataset\n",
    "data_original = data.copy()\n",
    "\n",
    "#Encode the column for Sex (female:0, male:1)\n",
    "gender_mapping = {\"female\":0,\"male\":1}\n",
    "data['Sex'] = data['Sex'].map( gender_mapping ).astype(int)\n",
    "\n",
    "#Encode the column for AgeGroup\n",
    "age_mapping = {\"Child\":0,\"Adolescent\":1,\"Adult\":2,\"Middle Aged\":3,\"Elderly\":4}\n",
    "data['AgeGroup'] = data['AgeGroup'].map( age_mapping ).astype(int)\n",
    "\n",
    "#Encode the column for titles \n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\n",
    "data['Title'] = data['Title'].map(title_mapping).astype(int)\n",
    "data['Title'] = data['Title'].fillna(0)\n",
    "\n",
    "#Encode the column for Embarked\n",
    "embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}\n",
    "data['Embarked'] = data['Embarked'].map(embarked_mapping).astype(int)\n",
    "\n",
    "#Changing data type of Fare and Age to integer\n",
    "fare_mapping = {\"Cheap\":0,\"Lower-Mid\":1,\"Upper-Mid\":2,\"High\":3}\n",
    "data['FareCategory'] = data['FareCategory'].map(fare_mapping).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peeking at our dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Let's drop the columns we no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of columns to drop\n",
    "drop_elements = ['Name','SibSp','Parch','Age','Fare']\n",
    "#Drop columns\n",
    "data.drop(drop_elements, axis = 1, inplace=True)\n",
    "\n",
    "#peeking at our dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some exploratory analysis and get some descriptive statistics and graphs out of the data.\n",
    "\n",
    "**Exercise**  \n",
    "Write code to find the survival rate between every category amongst the predictor variables and the response variable.\n",
    "\n",
    "Eg.   \n",
    "Predictor Variable: Pclass  \n",
    "Survival Rate with Response Variable (Survived):  \n",
    "{1: 0.62, 2: 0.47, 3: 0.24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the survival rate between between different categories amongst our predictor variables\n",
    "for x in data.columns.tolist()[1:]:\n",
    "    if data[x].dtype != 'float64' :\n",
    "        print('Survival Rate by:', x)\n",
    "        #TODO: Find and print the survival rate for every predictor variable \n",
    "        ?\n",
    "        print('-'*50, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a correlation heatmap for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "Machine Learning Algorithms dont understand classes like we do. There is a risk our integer encoding for categorical variables can be misinterpreted in a way that a higher integer is more important. For example, both genders should be percieved as equal but due to male being 1 and female being 0, there is a risk that more weight is allocated to the male geneder during analysis. For this we do one hot encoding. This will create an additional column for each row&mdash;one for female with a binary response of 0 or 1 and one for male with a binary response for 0 or 1. We will do this encoding for all our categorical variables:\n",
    "\n",
    "- Sex\n",
    "- Embarked\n",
    "- AgeGroup\n",
    "- FamilySize\n",
    "- isAlone\n",
    "- Title\n",
    "\n",
    "\n",
    "We are not doing this encoding for ordinal variables like Pclass and FareCategory because there is an order to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this point, let's create another copy of our data\n",
    "data_encoded = data.copy()\n",
    "\n",
    "#one hot encoding\n",
    "data = pd.get_dummies(data, columns=[\"Sex\",\"Embarked\",\"AgeGroup\",\"FamilySize\",\"isAlone\",\"Title\"], \\\n",
    "                         prefix=[\"Sex\", \"Embarked\", \"AgeGroup\",\"FamilySize\",\"isAlone\",\"Title\"])\n",
    "\n",
    "\n",
    "#taking a peek\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 1782 passengers on the titanic. Let's restructure our training and test sets, we'll allocate approximately 80% passengers to our training set and 20% to our test set.\n",
    "\n",
    "#### Exercise:  \n",
    "Split the data into _training_ and _test_ sets.  \n",
    "_Hint: Look up scikit-learn's train_test_split function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the features into a variable X [numpy array] and the dependent variable into y [numpy array]\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "\n",
    "#TODO: Split dataset into train, test\n",
    "#Test size should be 20% of the total data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = ?\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}\\n\\\n",
    "Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**:  \n",
    "```\n",
    "Shape of X_train: (1425,28), Shape of y_train: (1425,)  \n",
    "Shape of X_test: (257, 28), Shape of y_test: (357,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Let's begin with using sci-kit learn to just directly put our data into a logistic regression model and see our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "scikit_log_reg = LogisticRegression(penalty=\"l2\")\n",
    "scikit_log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Score is Mean Accuracy\n",
    "scikit_score = scikit_log_reg.score(X_test,y_test)\n",
    "print(f'Accuracy: {round(scikit_score*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset. Calculating some other performance metrics can give us a better idea about how our classification model has performed.\n",
    "\n",
    "- _Accuracy_: Out of all our predictions, what percentage were correct. \n",
    "- _True Positives_: Out of all the people we predicted will survive, how many actually survived.  \n",
    "- _False Positives_: Out of all the people we predicted will survive, how many did not survive.  \n",
    "- _False Negatives_: Out of all the people we predicted will not survive, how many actually survived.\n",
    "- _True Negatives_: Out of all the people we predicted will not survive, how many actually did not survive.\n",
    "\n",
    "\n",
    "- _Recall_: Out of all the people who survived, what percentage did we predict will survive.   \n",
    "\n",
    "    $$ Recall = {TP \\over {TP + FN}}$$\n",
    "    \n",
    "    \n",
    "- _Precision_: Out of all the people we predicted have survived, what percentage actually survived.  \n",
    "\n",
    "    $$ Precision = {TP \\over {TP + FP}}$$\n",
    "    \n",
    "    \n",
    "- _$F_1$ Score_: In binary statistical analysis, the F score is used as a measure to convey the balance between precision and recall. An $F_1$ score reaches it's best value at 1 (perfect precision and recall) and worst at 0.\n",
    "\n",
    "    $$ F_1 = 2  \\times  {{precision \\times recall}\\over{precision + recall}}$$\n",
    "    \n",
    "    \n",
    "- _Confusion Matrix_: A confusion matrix is a technique for summarizing the performance of a classification algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "Calculate:  \n",
    "1. Precision\n",
    "2. Recall\n",
    "3. $F_1$ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use our Logistic Regression Model to get an array of predictions\n",
    "predictions = scikit_log_reg.predict(X_test)\n",
    "\n",
    "#Function to calculate Precision and Recall\n",
    "def precision_recall_f1(y_pred, y):\n",
    "    '''\n",
    "    Arguments:\n",
    "        y_pred - array of prediction values of shape (m,1) \n",
    "        y      - array of actual values of shape (m, 1)\n",
    "    Output:\n",
    "        Returns a dictionary containing the precision, recall, and F1 score\n",
    "    '''\n",
    "    \n",
    "    #TODO: Calculate True positives, false positives, false negatives, true positives\n",
    "    ?\n",
    "    \n",
    "    #Calculate precision\n",
    "    precision = TP/(TP+FP)\n",
    "    #Calculate recall\n",
    "    recall = TP/(TP+FN)\n",
    "    #Calculate F1 score\n",
    "    f1 = 2*((precision*recall)/(precision+recall))\n",
    "        \n",
    "    return {\"Precision\":precision, \"Recall\":recall, \"F1\":f1}\n",
    "\n",
    "scores = precision_recall_f1(predictions,y_test)\n",
    "print(f\"Precision: {scores['Precision']}\\n\\\n",
    "Recall: {scores['Recall']}\\n\\\n",
    "F1: {scores['F1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt=\"g\"); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['P', 'N']); ax.yaxis.set_ticklabels(['P', 'N']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Let's look at another way to model the data using Logistic Regression. But this time, we'll code in the optimization algorithm ourselves using an optimization algorithm called Gradient Descent.\n",
    "\n",
    "### What is a Gradient? \n",
    "A gradient is a fancy word for a derivative. And a derivative is the rate of change of a function. It is a vector that:\n",
    "- Points in the direction of the greatest increase of a function.\n",
    "- Is zero at a local maximum or minimum (because there is no single direction of increase).\n",
    "\n",
    "Now, just like Linear Regression, we want the best weights and biases (slope and intercept terms) which help us have the least distance for the correct class and the most distance for the incorrect class.\n",
    "\n",
    "For that we measure the loss:\n",
    "$$ L = {{1 \\over {N}} \\sum_i D(S(wx_i + b), L_i)}$$\n",
    "Here, $S$ is our prediction vector and $L_i$ are the actual labels.\n",
    "\n",
    "Our aim is to minimize this function. One of the simplest and best way to minimize the loss is **Gradient Descent**. Take the derivative of your loss with respect to your parameters and follow that derivative by taking a step backward and repeat until you are at the minima.\n",
    "\n",
    "### Hill Climbing\n",
    "Let's break down gradient descent outside of the technical talk. Imagine you're walking in a hilly region, and you need to get down from the hills towards a valley. You dont really care that much which valley you get down to&mdash;although you'd prefer the valley at the very bottom&mdash;but you'll make do with any valley. Unfortunately, you're blind and you cant visually see where the vallies are located. But you can _feel_ whether you're going up or down. \n",
    "\n",
    "Now let's say you're standing at a random position in this hilly area and you take a step in one direction. Also assume that you have superpowers so you can take _really long_ steps. You want to make sure the steps you take are not _too small_, in which case you'll take years to reach the valley. But they shouldnt be _too large_ either, othewise you'd end up walking over a valley and not realize it (like I said, you can take _really_ large steps if you want). So you'd better be wise about your definition of your _step taking rate_&mdash;or in machine learning parlance: **learning rate**.\n",
    "\n",
    "Right, so you take a step and you _feel_ you've come down a little bit from the earlier position you were at. In machine learning terms, you will _feel_ you've come down when you measure you're performance using the **loss function**. If the loss function has come down from the previous step, then it means you're going in the right direction. At this point, you will take the gradient of the loss function and update your original weights and biases. \n",
    "\n",
    "Now you take another step from the updated position! And so on and so forth you continue for **n iterations** _or_ till you reach a valley. If you reach a valley, the gradient will keep oscillating within that valley and you will not go further down.\n",
    "\n",
    "<img src =\"images/gradient_descent.png\">\n",
    "\n",
    "**Exercise**  \n",
    "Let's get to work to find the right linear combination of weights and inputs which will help us classify these two types of iris plants correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by creating a function to initialize the weights\n",
    "#For our first attempt let's initialize the weights to 0\n",
    "def initialize_weights(features):\n",
    "    '''\n",
    "    Arguments:\n",
    "        features -- input matrix (n, m) for our feature set\n",
    "            n - number of training examples\n",
    "            m - number of predictor variables\n",
    "    Output:\n",
    "        Returns a matrix of shape (m, 1)\n",
    "    '''\n",
    "    #TODO: Create a random matrix (preferably from a standard normal distribution)\n",
    "    #of shape (m,1)\n",
    "    ?\n",
    "    \n",
    "#Let's add in the intercept terms to our input matrix.\n",
    "#The intercept term is the garbage collector of our model.\n",
    "#It accounts for any bias that is not accounted for by the terms in the model.\n",
    "def add_intercept(features):\n",
    "    '''\n",
    "    Arguments:\n",
    "        features -- input matrix (n x m) for our feature set\n",
    "    Output:\n",
    "        Returns an input matrix of shape (n x m+1) with a column\n",
    "        of 1's appended to the matrix\n",
    "    '''\n",
    "    #TODO: Create a vector of 1's of shape (n, 1)\n",
    "    #append vector to the feature set\n",
    "    ?\n",
    "    \n",
    "#Adding the intercept term\n",
    "X_ = add_intercept(X)\n",
    "#Initializing the weights matrix\n",
    "theta_ = initialize_weights(X_)\n",
    "\n",
    "#Let's look at the shapes of our input matrix X and\n",
    "#our weights matrix theta\n",
    "print(f\"Shape of Input Matrix X: {X_.shape}\")\n",
    "print(f\"Shape of Weights Matrix theta: {theta_.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "```\n",
    "Shape of Input Matrix X: (1782, 29)  \n",
    "Shape of Weights Matrix theta: (29, 1)\n",
    "```\n",
    "Now, let's creat the predict function. It will find the dot product between the weights matrix and the input matrix and then use the sigmoid function to find the probability of each input row of being part of class 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "    '''\n",
    "    Arguments:\n",
    "        features - (n,m) matrix of features\n",
    "        weights - (m,1) matrix of weights\n",
    "    Output:\n",
    "        returns a (n,1) vector of predictions\n",
    "\n",
    "    '''    \n",
    "    \n",
    "    #TODO: Find the dot product of the weights and the feature set\n",
    "    #Apply the sigmoid function to the output\n",
    "    ?\n",
    "\n",
    "#Let's test it and see what we get\n",
    "b = np.random.randn(3,3)\n",
    "w = np.random.normal(0,1,(3,1))\n",
    "predictions_ = predict(b,w)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**. \n",
    "```\n",
    "[[0.74056795]\n",
    " [0.18281486]\n",
    " [0.14084556]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "The cost funtion is used to identify how good our prediction for a certain sample or a set of samples actually is when compared with the correct classifications. Our prediction is non-linear due to the sigmoid function, hence, we cannot use the L2 loss function. Instead we use the **Cross-Entropy** (also known as Log Loss) function. It can be thought of as a combination of two separate loss functions, one for $y=1$ and one for $y=0$.\n",
    "Cost of a single example:\n",
    "<table><tr><td width = \"70%\">\n",
    "$$L_{cost} = {-y\\log{1\\over{1+e^{-\\theta^{T}x}}}} - {(1-y)\\log{1-{1\\over{1+e^{-\\theta^T x}}}}}$$</td><td width = \"30%\">\t$$\\begin{cases}\n",
    "            \\text{if } y=1, \\text{we want } \\theta^T x >> 0 \\\\\n",
    "            \\text{if } y=0, \\text{we want } \\theta^T x << 0\n",
    "            \\end{cases}$$</td></tr>\n",
    "            \n",
    "</table>\n",
    "\n",
    "Total Cost:\n",
    "$$J(\\theta) = min_\\theta {1\\over{m}} \\Bigl [ \\sum^m_{i=1} y^{(i)} (-log \\ h_\\theta(x^{(i)})) + (1-y^{(i)}) \\ ((-log(1-h_\\theta(x^{(i)}))) \\Bigr ]$$\n",
    "\n",
    "The benefits of using these loss functions is revealed when we look at their graphs.\n",
    "\n",
    "### Regularization\n",
    "To prevent over-fitting, we add a regularization term which can be thought of as a penalty term on our loss function. It discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
    "\n",
    "Our cost function with regularization becomes:\n",
    "\n",
    "$$J(\\theta) = min_\\theta {1\\over{m}} \\Bigl [ \\sum^m_{i=1} y^{(i)} (-log \\ h_\\theta(x^{(i)})) + (1-y^{(i)}) \\ ((-log(1-h_\\theta(x^{(i)}))) \\Bigr ] + {\\lambda\\over{2m}} \\ {{\\sum^n_{i=1} \\theta^2_j}} $$\n",
    "$$\\text{where } \\lambda \\text{ is the regularization parameter}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the loss function for y == 1\n",
    "def loss_y1(x): return -np.log(x)\n",
    "#Defining the loss function for y == 0\n",
    "def loss_y2(x): return -np.log(1-x)\n",
    "\n",
    "#Calculating the loss\n",
    "a1_loss = loss_y1(z1)\n",
    "a2_loss = loss_y2(z1)\n",
    "\n",
    "#Plot\n",
    "f = plt.figure(figsize=(13,5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(z1,a1_loss)\n",
    "ax1.set_xlabel(\"z\")\n",
    "ax1.set_ylabel(\"loss\")\n",
    "ax2.plot(z2,a2_loss)\n",
    "ax2.set_xlabel(\"z\")\n",
    "ax2.set_ylabel(\"loss\")\n",
    "ax1.set_title(\"Loss for y == 1\")\n",
    "ax2.set_title(\"Loss for y == 0\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These smooth monotonic functions (always increasing or decreasing) make it easy to calculate the gradient and minimize cost.\n",
    "\n",
    "**Exercise**  \n",
    "Create a function for the cross entropy loss function (with regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The loss/cost function\n",
    "def cost_function(features, labels, weights, reg):\n",
    "    '''\n",
    "    Using Mean Absolute Error\n",
    "\n",
    "    Features:(m,n)\n",
    "    Labels: (m,1)\n",
    "    Weights:(n,1)\n",
    "    Returns 1D matrix of predictions\n",
    "    Cost = ( log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n",
    "    '''\n",
    "    observations = len(labels)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #TODO: Take the error when label=1\n",
    "    class1_cost = ?\n",
    "\n",
    "    #TODO: Take the error when label=0\n",
    "    class2_cost = ?\n",
    "    \n",
    "    #TODO: Add the regularization term\n",
    "    regularization_term = ?\n",
    "    \n",
    "    #Take the sum of both costs\n",
    "    cost = class1_cost + class2_cost + regularization_term\n",
    "\n",
    "    #Take the average cost\n",
    "    cost = cost.sum()/observations\n",
    "\n",
    "    return cost\n",
    "\n",
    "#Let's test it and see what we get\n",
    "np.random.seed(420)\n",
    "b = np.random.randn(3,3)\n",
    "w = np.random.normal(0,1,(3,1))\n",
    "p = np.array([0,1,0])\n",
    "print(f\"Cost of one iteration: {cost_function(b,p,w,0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "Cost of one iteration: 2.655157288371231\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "The size of these steps is called the learning rate. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.\n",
    "$$ {\\delta{J(\\theta)}\\over{\\delta\\theta_j}} = {1\\over{m}} \\ X^T (g(X\\theta) - y)$$\n",
    "where:  \n",
    "m -- number of observations  \n",
    "X -- feature vector  \n",
    "y -- labels  \n",
    "$\\theta$ -- weights  \n",
    "g(X$\\theta$) -- predictions  \n",
    "\n",
    "**Exercise**  \n",
    "Write the function to update the weights after taking one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent\n",
    "def update_weights(features, labels, weights, lr):\n",
    "    '''\n",
    "    Vectorized Gradient Descent\n",
    "\n",
    "    Features:(200, 3)\n",
    "    Labels: (200, 1)\n",
    "    Weights:(3, 1)\n",
    "    '''\n",
    "    N = len(features)\n",
    "\n",
    "    # TODO: Get Predictions\n",
    "    predictions = ?\n",
    "\n",
    "    # TODO: Transpose features from (n, m) to (m, n)\n",
    "    # So we can multiply w the (n,1)  cost matrix.\n",
    "    # Returns a (m,1) matrix holding m partial derivatives --\n",
    "    # one for each feature -- representing the aggregate\n",
    "    # slope of the cost function across all observations\n",
    "    gradient = ?\n",
    "\n",
    "    # Take the average cost derivative for each feature\n",
    "    gradient /= N\n",
    "\n",
    "    # Multiply the gradient by our learning rate\n",
    "    gradient *= lr\n",
    "\n",
    "    # Subtract from our weights to minimize cost\n",
    "    weights -= gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "#Let's test it and see what we get\n",
    "np.random.seed(420)\n",
    "b = np.random.randn(3,5)\n",
    "w = np.random.normal(0,1,(5,1))\n",
    "p = np.array([0,1,0]).reshape(-1,1)\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(update_weights(b,p,w,learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "[[ 0.38277304]\n",
    " [-0.69288939]\n",
    " [ 0.71153   ]\n",
    " [-0.34105048]\n",
    " [ 1.54394326]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the decision boundary\n",
    "def decision_boundary(prob):\n",
    "    return 1 if prob > 0.5 else 0\n",
    "\n",
    "#Converting probabilities to classes \n",
    "def classify(preds):\n",
    "    '''\n",
    "    input - N element array of predictions between 0 and 1\n",
    "    output - N element array of 0's (false) and 1's (true)\n",
    "    '''\n",
    "    #referencing our decision boundary function\n",
    "    db = np.vectorize(decision_boundary)\n",
    "    return db(preds).flatten()\n",
    "\n",
    "#Example output\n",
    "predictions_ = predict(b,w)\n",
    "classifications = classify(predictions_)\n",
    "classifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**  \n",
    "Write a function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training code\n",
    "def train(features, labels, lr, iters, reg):\n",
    "    cost_history = []\n",
    "    #TODO: Add intercept term to features\n",
    "    features = ?\n",
    "    #TODO: Initialize weights\n",
    "    weights = ?\n",
    "    \n",
    "    for i in range(iters):\n",
    "        #TODO: update the weights\n",
    "        weights = ?\n",
    "        \n",
    "        #TODO: Calculate error\n",
    "        cost = ?\n",
    "        #append error to cost history\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        #Log progress\n",
    "        if i%1000 == 0:\n",
    "            print(f\"iteration: {i}, cost: {cost}\")\n",
    "    \n",
    "    plt.plot(list(range(iters)), cost_history)\n",
    "    plt.title(\"Loss Function curve\")\n",
    "    plt.xlabel(\"Iterations\"); plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    return weights, cost_history\n",
    "\n",
    "W, cost_history = train(X_train, y_train.reshape(-1,1), lr = 0.01, iters = 100000, reg = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "def accuracy(predicted_labels, actual_labels):\n",
    "    diff = predicted_labels - actual_labels\n",
    "    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))\n",
    "\n",
    "predictions = classify(predict(add_intercept(X_test),W))\n",
    "print(f\"Accuracy of our classifier: {round(accuracy(predictions.reshape(predictions.shape[0],1),y_test.reshape(-1,1)),5)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt=\"g\"); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['P', 'N']); \n",
    "ax.yaxis.set_ticklabels(['P', 'N']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = precision_recall_f1(predictions,y_test)\n",
    "print(f\"Precision: {scores['Precision']}\\n\\\n",
    "Recall: {scores['Recall']}\\n\\\n",
    "F1: {scores['F1']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
